

import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torch.utils.tensorboard import SummaryWriter

# 超参数设置
lr = 0.1
batch_size = 128
epochs = 100
log_interval = 100

# 选择设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 优化器和学习率调度器
optimizer_cnn = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)
scheduler_cnn = StepLR(optimizer_cnn, step_size=30, gamma=0.1)

optimizer_vit = optim.SGD(vit_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)
scheduler_vit = StepLR(optimizer_vit, step_size=30, gamma=0.1)

# 损失函数
criterion = nn.CrossEntropyLoss()

# TensorBoard 记录
writer = SummaryWriter()

# 训练和验证函数
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        data, targets_a, targets_b, lam = cutmix(data, target)
        optimizer.zero_grad()
        output = model(data)
        loss = lam * criterion(output, targets_a) + (1 - lam) * criterion(output, targets_b)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')
            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)

def validate(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum
